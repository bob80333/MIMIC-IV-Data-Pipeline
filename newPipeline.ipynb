{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "\n",
    "module_path='preprocessing/day_intervals_preproc'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "module_path='utils'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "module_path='preprocessing/hosp_module_preproc'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "module_path='model'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "root_dir = \"/data/corpora_alpha/MIMIC/MIMIC_IV_2.2/files\"\n",
    "\n",
    "import day_intervals_cohort\n",
    "from day_intervals_cohort import *\n",
    "\n",
    "import day_intervals_cohort_v2\n",
    "from day_intervals_cohort_v2 import *\n",
    "\n",
    "import data_generation_icu\n",
    "\n",
    "import data_generation\n",
    "import evaluation\n",
    "\n",
    "import feature_selection_hosp\n",
    "from feature_selection_hosp import *\n",
    "\n",
    "# import train\n",
    "# from train import *\n",
    "\n",
    "\n",
    "import ml_models\n",
    "from ml_models import *\n",
    "\n",
    "import dl_train\n",
    "from dl_train import *\n",
    "\n",
    "import tokenization\n",
    "from tokenization import *\n",
    "\n",
    "\n",
    "import behrt_train\n",
    "from behrt_train import *\n",
    "\n",
    "import feature_selection_icu\n",
    "from feature_selection_icu import *\n",
    "import fairness\n",
    "import callibrate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(day_intervals_cohort)\n",
    "import day_intervals_cohort\n",
    "from day_intervals_cohort import *\n",
    "\n",
    "importlib.reload(day_intervals_cohort_v2)\n",
    "import day_intervals_cohort_v2\n",
    "from day_intervals_cohort_v2 import *\n",
    "\n",
    "importlib.reload(data_generation_icu)\n",
    "import data_generation_icu\n",
    "importlib.reload(data_generation)\n",
    "import data_generation\n",
    "\n",
    "importlib.reload(feature_selection_hosp)\n",
    "import feature_selection_hosp\n",
    "from feature_selection_hosp import *\n",
    "\n",
    "importlib.reload(feature_selection_icu)\n",
    "import feature_selection_icu\n",
    "from feature_selection_icu import *\n",
    "\n",
    "importlib.reload(tokenization)\n",
    "import tokenization\n",
    "from tokenization import *\n",
    "\n",
    "importlib.reload(ml_models)\n",
    "import ml_models\n",
    "from ml_models import *\n",
    "\n",
    "importlib.reload(dl_train)\n",
    "import dl_train\n",
    "from dl_train import *\n",
    "\n",
    "importlib.reload(behrt_train)\n",
    "import behrt_train\n",
    "from behrt_train import *\n",
    "\n",
    "importlib.reload(fairness)\n",
    "import fairness\n",
    "\n",
    "importlib.reload(callibrate_output)\n",
    "import callibrate_output\n",
    "\n",
    "importlib.reload(evaluation)\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda_basic(dataframes, dataframes_names = None, log_path=None):\n",
    "    '''\n",
    "    Perform basic exploratory data analysis on a list of pandas DataFrames.\n",
    "    Args:\n",
    "    dataframes (list): A list of pandas DataFrames.\n",
    "    dataframes_names (list, optional): A list of names for the DataFrames. If provided, the names will be used to distinguish between the DataFrames in the results.\n",
    "    log_path (str, optional): The path to save the log file. If provided, the results will be logged to the file.\n",
    "    '''\n",
    "    if(len(dataframes) == 0):\n",
    "        print(\"No dataframes provided.\")\n",
    "        return\n",
    "    \n",
    "    if dataframes_names:\n",
    "        if len(dataframes) != len(dataframes_names):\n",
    "            print(\"The number of dataframes and the number of names provided do not match.\")\n",
    "            return\n",
    "\n",
    "    if log_path:\n",
    "        log_file_path = os.path.join(log_path, 'eda_basic_log.txt')\n",
    "        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "        log_file = open(log_file_path, 'a')\n",
    "        log_file.write(f\"{'='*40}\\n\")\n",
    "        log_file.write(f\"{pd.Timestamp.now()}\\n\")\n",
    "        log_file.write(f\"{'='*40}\\n\")\n",
    "        \n",
    "    for df_index in range(len(dataframes)):\n",
    "        if dataframes_names:\n",
    "            print(f\"{'*'*40}\\n{dataframes_names[df_index]}\\n{'*'*40}\")\n",
    "            if log_path:\n",
    "                log_file.write(f\"{'*'*40}\\n{dataframes_names[df_index]}\\n{'*'*40}\\n\")\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None):\n",
    "            results = [\n",
    "                (\"First few rows of the dataframe:\\n\", dataframes[df_index].head()),\n",
    "                (\"Columns in the dataframe:\\n\", dataframes[df_index].columns),\n",
    "                (\"Number of rows in the dataframe:\\n\", dataframes[df_index].shape[0]),\n",
    "                (\"Missing values in the dataframe:\\n\", dataframes[df_index].isnull().sum()),\n",
    "                (\"Number of duplicate rows in the dataframe:\\n\", dataframes[df_index].duplicated().sum()),\n",
    "                (\"Data types of columns:\\n\", dataframes[df_index].dtypes),\n",
    "                (\"Number of unique values in each column:\\n\", dataframes[df_index].nunique()),\n",
    "            ]\n",
    "            for label, result in results:\n",
    "                print(label)\n",
    "                print(result)\n",
    "                print()\n",
    "                if log_path:\n",
    "                    log_file.write(f\"{label}{result}\\n\\n\")\n",
    "            if log_path:\n",
    "                log_file.write(f\"{'X'*40}\\n\")\n",
    "    if log_path:\n",
    "        log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation_cohort_var_non_ICU = \"cohort_non-icu_imputation\"\n",
    "imputation_cohort_var_ICU = \"cohort_icu_imputation\"\n",
    "imputation_cohort_dataset_non_ICU = pd.read_csv(f\"./data/cohort/{imputation_cohort_var_non_ICU}.csv.gz\")\n",
    "imputation_cohort_dataset_ICU = pd.read_csv(f\"./data/cohort/{imputation_cohort_var_ICU}.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXTRACTING PROCEDURES DATA]\n",
      "# Unique ICD9 Procedures:   2544\n",
      "# Unique ICD10 Procedures:  10031\n",
      "\n",
      "Value counts of each ICD version:\n",
      " icd_version\n",
      "9     446079\n",
      "10    223107\n",
      "Name: count, dtype: int64\n",
      "# Admissions:   229445\n",
      "Total number of rows:  669186\n",
      "[SUCCESSFULLY SAVED PROCEDURES DATA]\n"
     ]
    }
   ],
   "source": [
    "diag_flag = False\n",
    "lab_flag = False\n",
    "proc_flag = True\n",
    "med_flag = False\n",
    "cohort_output = imputation_cohort_var_non_ICU\n",
    "version_path = \"mimiciv/2.2\"\n",
    "feature_nonicu_imputation(cohort_output,version_path,diag_flag,lab_flag,proc_flag,med_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "Procedures\n",
      "****************************************\n",
      "First few rows of the dataframe:\n",
      "\n",
      "   subject_id   hadm_id  seq_num   chartdate icd_code  icd_version  \\\n",
      "0    10000032  22595853        1  2180-05-07     5491            9   \n",
      "1    10000032  22841357        1  2180-06-27     5491            9   \n",
      "2    10000032  25742920        1  2180-08-06     5491            9   \n",
      "3    10000068  25022803        1  2160-03-03     8938            9   \n",
      "4    10000117  27988844        1  2183-09-19  0QS734Z           10   \n",
      "\n",
      "             admittime            dischtime  \\\n",
      "0  2180-05-06 22:23:00  2180-05-07 17:15:00   \n",
      "1  2180-06-26 18:27:00  2180-06-27 18:49:00   \n",
      "2  2180-08-05 23:44:00  2180-08-07 17:50:00   \n",
      "3  2160-03-03 23:16:00  2160-03-04 06:26:00   \n",
      "4  2183-09-18 18:10:00  2183-09-21 16:30:00   \n",
      "\n",
      "                                          long_title proc_time_from_admit  \n",
      "0                    Percutaneous abdominal drainage      0 days 01:37:00  \n",
      "1                    Percutaneous abdominal drainage      0 days 05:33:00  \n",
      "2                    Percutaneous abdominal drainage      0 days 00:16:00  \n",
      "3        Other nonoperative respiratory measurements    -1 days +00:44:00  \n",
      "4  Reposition Left Upper Femur with Internal Fixa...      0 days 05:50:00  \n",
      "\n",
      "Columns in the dataframe:\n",
      "\n",
      "Index(['subject_id', 'hadm_id', 'seq_num', 'chartdate', 'icd_code',\n",
      "       'icd_version', 'admittime', 'dischtime', 'long_title',\n",
      "       'proc_time_from_admit'],\n",
      "      dtype='object')\n",
      "\n",
      "Number of rows in the dataframe:\n",
      "\n",
      "669186\n",
      "\n",
      "Missing values in the dataframe:\n",
      "\n",
      "subject_id              0\n",
      "hadm_id                 0\n",
      "seq_num                 0\n",
      "chartdate               0\n",
      "icd_code                0\n",
      "icd_version             0\n",
      "admittime               0\n",
      "dischtime               0\n",
      "long_title              0\n",
      "proc_time_from_admit    0\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows in the dataframe:\n",
      "\n",
      "0\n",
      "\n",
      "Data types of columns:\n",
      "\n",
      "subject_id               int64\n",
      "hadm_id                  int64\n",
      "seq_num                  int64\n",
      "chartdate               object\n",
      "icd_code                object\n",
      "icd_version              int64\n",
      "admittime               object\n",
      "dischtime               object\n",
      "long_title              object\n",
      "proc_time_from_admit    object\n",
      "dtype: object\n",
      "\n",
      "Number of unique values in each column:\n",
      "\n",
      "subject_id              121891\n",
      "hadm_id                 229445\n",
      "seq_num                     41\n",
      "chartdate                35117\n",
      "icd_code                 12575\n",
      "icd_version                  2\n",
      "admittime               224869\n",
      "dischtime               225726\n",
      "long_title               12575\n",
      "proc_time_from_admit     35539\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "procedure_df = pd.read_csv(f\"./data/features/imputation_non_icu_preproc_proc.csv.gz\")\n",
    "eda_basic([procedure_df], ['Procedures'], './data/summary/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1_mimic_iv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
